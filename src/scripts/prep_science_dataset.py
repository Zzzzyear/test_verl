import os
import glob
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from datasets import load_dataset, Dataset
from tqdm import tqdm
import multiprocessing

# ================= 1. ÈÖçÁΩÆÂå∫ =================
# Ëá™Âä®ÂØªÊâæÈ°πÁõÆÊ†πÁõÆÂΩï
def find_project_root():
    candidates = [
        "/data-store/zhaoqiannian/workspace/EGPO",
        "/data/zhaoqn/workspace/EGPO",
        os.getcwd()
    ]
    for path in candidates:
        if os.path.exists(path): return path
    return os.getcwd()

BASE_DIR = find_project_root()
RAW_DIR = os.path.join(BASE_DIR, "datasets/raw")
PROCESSED_DIR = os.path.join(BASE_DIR, "datasets/processed")
os.makedirs(PROCESSED_DIR, exist_ok=True)

# ÁõÆÊ†áËÆæÂÆö
TARGET_SCIENCE_COUNT = 200000  # ÁõÆÊ†á 20‰∏á
VAL_SIZE = 256                 # È™åËØÅÈõÜÂ§ßÂ∞è (‰øùÊåÅ‰∏éÂÖ∂‰ªñÂ§ßÊï∞ÊçÆÈõÜ‰∏ÄËá¥)
DUMMY_MESSAGE = [{"role": "user", "content": "DUMMY_SCHEMA_FIX"}]

# ================= 2. Ê†∏ÂøÉÂ§ÑÁêÜÈÄªËæë (Â§çÁî® prep_dataset.py) =================

def to_chat_list(sys, user, asst=None):
    msgs = []
    if sys: msgs.append({"role": "system", "content": str(sys)})
    if user: msgs.append({"role": "user", "content": str(user)})
    if asst: msgs.append({"role": "assistant", "content": str(asst)})
    return msgs

def extract_boxed_answer(text):
    import re
    if not text: return None
    matches = re.findall(r"\\boxed\{(.*?)\}", text)
    if matches: return matches[-1] 
    match_mc = re.search(r"[Aa]nswer is\s?[:\s]?\s?([A-D])", text)
    if match_mc: return match_mc.group(1)
    return None

def process_row_science(ex):
    """‰∏ìÈó®Â§ÑÁêÜ Mixture ‰∏≠ÁöÑ Science Êï∞ÊçÆ"""
    res = {
        "data_source": "mixture",
        "ability": "science",
        "prompt": DUMMY_MESSAGE,
        "response": DUMMY_MESSAGE,
        "reward_model": {"style": "rule", "ground_truth": ""},
        "extra_info": {"split": "train", "index": 0, "ability": "science"},
        "is_valid": False,
    }

    # ÊèêÂèñÂéüÂßãÊ∂àÊÅØ
    msgs = ex.get('messages', [])
    if hasattr(msgs, 'tolist'): msgs = msgs.tolist()
    
    user_msg, asst_msg = "", ""
    if isinstance(msgs, list):
        for m in msgs:
            if isinstance(m, dict):
                role = m.get('role', '')
                content = m.get('content', '')
                if role == 'user': user_msg = str(content) if content else ""
                elif role == 'assistant': asst_msg = str(content) if content else ""
    
    # Ê†∏ÂøÉÊ†°È™å
    if user_msg and asst_msg:
        # 1. Á°Æ‰øùÊòØ Science Êù•Ê∫ê (ËôΩÁÑ∂Êàë‰ª¨‰ºöÂÖà filterÔºå‰ΩÜËøôÈáåÂÜçÈò≤‰∏ÄÈÅì)
        src = str(ex.get('source', '')).lower()
        
        # 2. ÊèêÂèñ Ground Truth
        clean_gt = extract_boxed_answer(asst_msg)
        
        if clean_gt:
            res.update({
                "prompt": to_chat_list(None, user_msg),
                "response": to_chat_list(None, None, asst_msg),
                "is_valid": True,
                "reward_model": {"style": "rule", "ground_truth": clean_gt}
            })

    return res

# ================= 3. PyArrow ÂÆâÂÖ®ÂàáÂàÜÂ∑•ÂÖ∑ (Â§çÁî® split_all_datasets.py) =================

def get_large_type(data_type):
    """ÈÄíÂΩíÂçáÁ∫ß‰∏∫ 64‰Ωç Large Á±ªÂûã"""
    if pa.types.is_string(data_type): return pa.large_string()
    if pa.types.is_binary(data_type): return pa.large_binary()
    if pa.types.is_list(data_type): return pa.large_list(get_large_type(data_type.value_type))
    if pa.types.is_fixed_size_list(data_type): return pa.fixed_size_list(get_large_type(data_type.value_type), data_type.list_size)
    if pa.types.is_struct(data_type):
        return pa.struct([field.with_type(get_large_type(field.type)) for field in data_type])
    if pa.types.is_map(data_type):
        return pa.map_(get_large_type(data_type.key_type), get_large_type(data_type.item_type))
    return data_type

def get_safe_schema(original_schema):
    new_fields = []
    for field in original_schema:
        new_type = get_large_type(field.type)
        new_fields.append(field.with_type(new_type))
    return pa.schema(new_fields)

def process_and_write(table, indices, output_path, chunk_size=1000, desc="Writing"):
    total = len(indices)
    if total == 0: return
    with pq.ParquetWriter(output_path, table.schema) as writer:
        for start in tqdm(range(0, total, chunk_size), desc=desc, unit="chunk"):
            end = min(start + chunk_size, total)
            batch_indices = indices[start:end]
            writer.write_table(table.take(batch_indices))

# ================= 4. ‰∏ªÊµÅÁ®ã =================

def main():
    print(f"üöÄ Starting Science Dataset Prep & Split")
    print(f"   Target: {TARGET_SCIENCE_COUNT} samples")
    print("=" * 60)

    # --- Step 1: Âä†ËΩΩ Mixture Êï∞ÊçÆ ---
    raw_files = glob.glob(os.path.join(RAW_DIR, "Mixture-of-Thoughts", "**/*.parquet"), recursive=True)
    if not raw_files:
        print("‚ùå Error: Mixture-of-Thoughts raw files not found!")
        return

    print("üìñ Loading Mixture-of-Thoughts...")
    ds = load_dataset("parquet", data_files=raw_files, split="train")
    
    # --- Step 2: Á≠õÈÄâ Science Êï∞ÊçÆ ---
    print("üîç Filtering for 'science' ability...")
    # Ê≥®ÊÑèÔºöMixture Êï∞ÊçÆÈõÜÈáå ability ÂàóÈÄöÂ∏∏ÊòØ 'math', 'code' Á≠âÔºåÊàë‰ª¨ÈúÄË¶ÅÁ°ÆËÆ§ 'science' Ê†áÁ≠æ
    # Ê†πÊçÆ prep_dataset.py ÈÄªËæëÔºöif 'math' or 'numina' -> math, else -> science (ÊéíÈô§ code)
    
    def is_science(x):
        src = str(x.get('source', '')).lower()
        if 'code' in src: return False
        if 'math' in src or 'numina' in src: return False
        # ÊéíÈô§ÊéâÊòéÁ°Æ‰∏çÊòØ Science ÁöÑÔºåÂâ©‰∏ãÁöÑÂΩì‰Ωú Science (ÂåÖÂê´ physics, chem, bio Á≠â)
        return True

    ds_sci = ds.filter(is_science, num_proc=16, desc="Filtering Science")
    print(f"   Found {len(ds_sci)} raw science candidates.")

    # --- Step 3: Ê†áÂáÜÂåñÂ§ÑÁêÜ ---
    print("‚öôÔ∏è  Standardizing format...")
    ds_processed = ds_sci.map(
        process_row_science,
        num_proc=16,
        remove_columns=ds.column_names,
        desc="Formatting"
    )
    ds_valid = ds_processed.filter(lambda x: x['is_valid'], desc="Dropping Invalid")
    print(f"   Valid Science Samples: {len(ds_valid)}")

    # --- Step 4: ÈááÊ†∑Ëá≥ 20‰∏á ---
    final_ds = ds_valid
    if len(final_ds) > TARGET_SCIENCE_COUNT:
        print(f"‚úÇÔ∏è  Downsampling to {TARGET_SCIENCE_COUNT}...")
        final_ds = final_ds.shuffle(seed=42).select(range(TARGET_SCIENCE_COUNT))
    
    # --- Step 5: ‰øùÂ≠ò Single Parquet (Ê∏ÖÊ¥ó Numpy) ---
    single_path = os.path.join(PROCESSED_DIR, "science_single.parquet")
    print(f"üíæ Saving intermediate: {single_path}")
    
    # ÈÄíÂΩíÊ∏ÖÊ¥ó Numpy (ÂÄüÁî® prep_dataset ÁöÑÈÄªËæë)
    def clean_obj_recursive(obj):
        if isinstance(obj, np.ndarray): return [clean_obj_recursive(x) for x in obj.tolist()]
        elif isinstance(obj, np.generic): return obj.item()
        elif isinstance(obj, list): return [clean_obj_recursive(x) for x in obj]
        elif isinstance(obj, dict): return {k: clean_obj_recursive(v) for k, v in obj.items()}
        return obj

    cleaned_list = [clean_obj_recursive(row) for row in tqdm(final_ds, desc="Sanitizing")]
    ds_safe = Dataset.from_list(cleaned_list)
    ds_safe.to_parquet(single_path)
    
    # --- Step 6: ÊâßË°åÂÆâÂÖ®ÂàáÂàÜ (Train/Val) ---
    print("\nüî™ Performing Safe Split (Train/Val)...")
    
    # ‰ΩøÁî® PyArrow ÂéüÁîüËØªÂèñ + Schema ÂçáÁ∫ß
    try:
        original_schema = pq.read_schema(single_path)
        safe_schema = get_safe_schema(original_schema)
        table = pq.read_table(single_path, schema=safe_schema)
        
        total_len = table.num_rows
        # Á¥¢ÂºïÊìç‰Ωú
        indices = np.arange(total_len)
        rng = np.random.default_rng(seed=42)
        rng.shuffle(indices)
        
        val_indices = indices[:VAL_SIZE]
        train_indices = indices[VAL_SIZE:]
        
        base_name = "science_single"
        train_out = os.path.join(PROCESSED_DIR, f"{base_name}_train_final.parquet")
        val_out = os.path.join(PROCESSED_DIR, f"{base_name}_val_fixed.parquet")
        
        print(f"   Writing Validation ({len(val_indices)})...")
        process_and_write(table, val_indices, val_out)
        
        print(f"   Writing Training ({len(train_indices)})...")
        process_and_write(table, train_indices, train_out)
        
        print("\n‚úÖ Science Dataset Pipeline Completed Successfully!")
        print(f"   -> {train_out}")
        print(f"   -> {val_out}")
        
    except Exception as e:
        print(f"‚ùå Critical Error during split: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    multiprocessing.set_start_method("fork", force=True)
    main()