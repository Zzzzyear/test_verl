import os
import argparse
import json
import re
import glob
import pandas as pd
import numpy as np
import subprocess
import sys
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


# ================= 1. æ·±åº¦å½’ä¸€åŒ–å·¥å…· =================

def normalize_math_str(s):
    """
    [å¢å¼ºç‰ˆ] æ•°å­¦å…¬å¼å½’ä¸€åŒ–ï¼Œå¯¹é½å­¦æœ¯ç•Œæ ‡å‡† (MATH/GSM8K)
    """
    if not s: return ""
    s = str(s).strip()

    # 1. åŸºç¡€æ¸…ç†
    s = s.replace("\n", " ").replace("\r", "")

    # 2. LaTeX å‘½ä»¤æ ‡å‡†åŒ–
    s = s.replace(r"\dfrac", r"\frac")
    s = s.replace(r"\tfrac", r"\frac")
    s = s.replace(r"\left", "").replace(r"\right", "")

    # 3. ç§»é™¤æ‰€æœ‰ç©ºæ ¼ (æ•°å­¦è¡¨è¾¾å¼é€šå¸¸å¯¹ç©ºæ ¼ä¸æ•æ„Ÿ)
    # æ³¨æ„ï¼šåœ¨æ–‡æœ¬é¢˜ä¸­å¯èƒ½è¦ä¿ç•™ï¼Œä½†åœ¨ Boxed Answer ä¸­é€šå¸¸åº”ç§»é™¤
    s = s.replace(" ", "")

    # 4. å¤„ç†è´§å¸ç¬¦å·å’Œå•ä½
    s = s.replace("$", "").replace("\\$", "")
    s = s.replace("%", "")

    # 5. ç§‘å­¦è®¡æ•°æ³•ç»Ÿä¸€ (1.2e-3 -> 1.2e-3, ä¿æŒåŸæ ·ï¼Œä¾èµ–åç»­ float å°è¯•)
    return s


def is_equiv_math(pred, gt):
    """
    [ä¸¥è°¨] æ•°å­¦ç­‰ä»·æ€§åˆ¤å®š
    1. å­—ç¬¦ä¸² Exact Match (å½’ä¸€åŒ–å)
    2. æ•°å€¼è¿‘ä¼¼ (å¯¹äºçº¯æ•°å­—)
    """
    norm_pred = normalize_math_str(pred)
    norm_gt = normalize_math_str(gt)

    # ç­–ç•¥ A: ä¸¥æ ¼å­—ç¬¦ä¸²åŒ¹é…
    if norm_pred == norm_gt:
        return True

    # ç­–ç•¥ B: æ•°å€¼è½¬æ¢åŒ¹é… (å…è®¸ 1e-6 è¯¯å·®)
    try:
        # ç§»é™¤å¸¸è§å¹²æ‰°è¯å†å°è¯•è½¬æ¢
        def to_float(x):
            x = x.replace("{", "").replace("}", "").replace("\\", "")
            return float(x)

        if abs(to_float(norm_pred) - to_float(norm_gt)) < 1e-6:
            return True
    except:
        pass

    return False


# ================= 2. ç­”æ¡ˆæå–å™¨ (å­¦æœ¯èŒƒå¼) =================

def clean_cot(text):
    """ç§»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹"""
    if not text: return ""
    # non-greedy match for <think>...</think>
    text = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL)
    return text.strip()


def extract_boxed_content(text):
    """
    [ä¿®å¤] åŸºäºè®¡æ•°å™¨çš„ Boxed æå–ï¼Œå®Œç¾æ”¯æŒåµŒå¥—
    å¯»æ‰¾æœ€åä¸€ä¸ª \boxed{...}
    """
    candidates = [m.start() for m in re.finditer(r"\\?boxed\s*\{", text)]
    if not candidates:
        return None

    # å–æœ€åä¸€ä¸ª boxed
    start_idx = candidates[-1]
    # æ‰¾åˆ° { çš„ä½ç½®
    brace_start = text.find("{", start_idx)
    if brace_start == -1: return None

    balance = 0
    content = []
    started = False

    for i in range(brace_start, len(text)):
        char = text[i]
        if char == "{":
            balance += 1
            started = True
        elif char == "}":
            balance -= 1

        if started:
            content.append(char)
            if balance == 0:
                break

    if len(content) >= 2:  # è‡³å°‘ "{}"
        return "".join(content[1:-1])  # å»æ‰å¤–å±‚ {}
    return None


def extract_math_answer(text):
    """Math æå–ç­–ç•¥ï¼šä¼˜å…ˆ Boxedï¼Œå…œåº•å–æœ€åä¸€è¡Œ"""
    text = clean_cot(text)

    # 1. å°è¯• Boxed (Standard)
    boxed = extract_boxed_content(text)
    if boxed: return boxed

    # 2. å…œåº•ï¼šå°è¯•æå–æœ€åä¸€è¡Œæ•°å€¼æˆ–ç®€çŸ­ç»“è®º
    # è¿™åœ¨ GSM8K Base æ¨¡å‹ä¸­å¸¸è§
    lines = text.split('\n')
    for line in reversed(lines):
        line = line.strip()
        if not line: continue
        # å¦‚æœåŒ…å« "The answer is", æˆªå–åé¢
        match = re.search(r"[Tt]he answer is[:\s]*(.+)", line)
        if match:
            return match.group(1).strip(" .")
        # å¦‚æœæ˜¯å¾ˆçŸ­çš„æ•°å­—/å…¬å¼
        if len(line) < 20 and any(c.isdigit() for c in line):
            return line
        break
    return ""


def extract_choice_answer(text):
    """
    [å¢å¼º] MCQ æå–ç­–ç•¥ï¼šæ˜¾å¼å£°æ˜ > Boxed > æ–‡æœ«å­—ç¬¦
    æ”¯æŒä¸­æ–‡ "ç­”æ¡ˆæ˜¯"
    """
    text = clean_cot(text)

    # 1. æ˜¾å¼å£°æ˜ (High Priority) - å–ç¬¬ä¸€ä¸ªåŒ¹é… (é˜²æ­¢åé¢çš„è§£é‡Šå¹²æ‰°)
    # å¢åŠ äº†ä¸­æ–‡æ”¯æŒ
    patterns = [
        r"(?:The|the) answer is[:\s]*\(?([A-Ja-j])\)?",
        r"(?:The|the) answer is[:\s]*(True|False|TRUE|FALSE|Yes|No|YES|NO)",
        r"ç­”æ¡ˆ(?:æ˜¯|ä¸º)[:\s]*\(?([A-Ja-j])\)?",
        r"é€‰é¡¹[:\s]*\(?([A-Ja-j])\)?",
    ]
    for p in patterns:
        match = re.search(p, text)
        if match: return match.group(1).strip()

    # 2. Boxed
    boxed = extract_boxed_content(text)
    if boxed:
        boxed = boxed.strip()
        if len(boxed) == 1 and boxed.isalpha(): return boxed
        if boxed.lower() in ['true', 'false', 'yes', 'no']: return boxed

    # 3. æ–‡æœ«å¼±åŒ¹é… (Low Priority) - å–æœ€åä¸€ä¸ªåŒ¹é…
    # ä»…åœ¨æœ«å°¾ 500 å­—ç¬¦æœç´¢
    last_part = text[-500:]
    match_letter = re.findall(r"\(?([A-Ja-j])\)", last_part)
    if match_letter: return match_letter[-1]

    return ""


def extract_code_block(text):
    """
    Code æå–ç­–ç•¥ï¼šå–ç¬¬ä¸€ä¸ªå®Œæ•´ä»£ç å— (å­¦æœ¯æ ‡å‡†)
    """
    text = clean_cot(text)

    # ä¼˜å…ˆ Python
    match = re.search(r"```python\s*(.*?)```", text, re.DOTALL)
    if match: return match.group(1)

    # å…¶æ¬¡é€šç”¨ä»£ç å—
    match = re.search(r"```\s*(.*?)```", text, re.DOTALL)
    if match: return match.group(1)

    # å…œåº•ï¼šå¦‚æœæ²¡ Markdownï¼Œè¿”å›å…¨æ–‡ (Base æ¨¡å‹å¯èƒ½ç›´æ¥è¾“å‡ºä»£ç )
    return text


# ================= 3. ä»£ç æ‰§è¡Œæ²™ç®± (å¢å¼ºç‰ˆ) =================

def run_python_io(code, test_input, timeout=3.0):
    """
    [æ–°å¢] é’ˆå¯¹ LCB/Codeforces çš„ IO æ¨¡å¼æ‰§è¡Œ
    é€šè¿‡ stdin æ³¨å…¥ inputï¼Œæ•è· stdout
    """
    try:
        # ä½¿ç”¨ sys.executable å¯åŠ¨å­è¿›ç¨‹
        process = subprocess.Popen(
            [sys.executable, "-c", code],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        stdout, stderr = process.communicate(input=test_input, timeout=timeout)
        if process.returncode != 0:
            return False, f"Error: {stderr.strip()}"
        return True, stdout.strip()
    except subprocess.TimeoutExpired:
        process.kill()
        return False, "Timeout"
    except Exception as e:
        return False, f"System Error: {e}"


def run_python_script(full_script, timeout=3.0):
    """
    é’ˆå¯¹ HumanEval çš„ Script æ¨¡å¼æ‰§è¡Œ
    """
    try:
        result = subprocess.run(
            [sys.executable, "-c", full_script],
            capture_output=True, text=True, timeout=timeout
        )
        if "EXECUTION_PASSED" in result.stdout:
            return True, "Passed"
        else:
            err = result.stderr if result.stderr else result.stdout
            return False, f"Failed: {err.strip()[-500:]}"
    except subprocess.TimeoutExpired:
        return False, "Timeout"
    except Exception as e:
        return False, f"System Error: {str(e)}"


# ================= 4. åˆ¤åˆ†è·¯ç”± (æ ¸å¿ƒ) =================

def check_sample(response, item):
    task_type = item['type']

    # --- 1. Code IO (LCB, Codeforces) ---
    if task_type == 'code_io':
        code = extract_code_block(response)
        if not code.strip(): return False, "No code extracted"

        inputs = item.get('test_inputs', [])
        outputs = item.get('test_outputs', [])

        if not inputs or not outputs:
            # å¦‚æœæ²¡æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œæ— æ³•åˆ¤åˆ† (LCBå¸¸è§æƒ…å†µ)
            # è¿”å› False ä½†æ ‡è®°ä¸º Skippedï¼Œæ–¹ä¾¿åˆ†æ
            return False, "Skipped (No test cases found in dataset)"

        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆè¯„æµ‹ï¼šåªè·‘å‰ 3 ä¸ª Case é˜²æ­¢å¤ªæ…¢
        # æ­£å¼è¯„æµ‹å»ºè®®ç”¨ä¸“é—¨çš„è¯„æµ‹åº“
        passed_cnt = 0
        for inp, outp in zip(inputs[:3], outputs[:3]):
            # æ ¼å¼åŒ–è¾“å…¥ï¼šå¦‚æœæ˜¯ listï¼Œé€šå¸¸ LCB è¾“å…¥æ˜¯å¤šè¡Œçš„ï¼Œæˆ–è€…æ˜¯ JSON string
            # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„ str è½¬æ¢ï¼Œå…·ä½“è§†æ•°æ®é›†æ ¼å¼è€Œå®š
            input_str = str(inp)
            ok, model_out = run_python_io(code, input_str)

            # å®½æ¾æ¯”å¯¹ (å»æ‰ç©ºç™½)
            if ok and model_out.split() == str(outp).strip().split():
                passed_cnt += 1
            else:
                return False, f"WA: Expected '{outp}', Got '{model_out}'"

        return True, "Passed samples"

    # --- 2. Code Script (HumanEval) ---
    elif task_type == 'code_script':
        code = extract_code_block(response)
        entry = item.get('entry', 'None')
        test_code = item['gt']

        # æ„é€ å®Œæ•´çš„æµ‹è¯•è„šæœ¬
        # å¢åŠ  assert é€»è¾‘é˜²æ­¢ entry_point ç©ºè½¬
        script = f"""
import sys
import math
import collections
import itertools
import random
import heapq
import functools
import re
from typing import *

{code}

{test_code}

try:
    # HumanEval æ ‡å‡† Check
    if '{entry}' != 'None' and 'check' in globals():
        check({entry})
    # LeetCode é£æ ¼: å¾€å¾€æ²¡æœ‰ check å‡½æ•°ï¼Œåªæœ‰ assert
    # æ‰€ä»¥åªè¦ä¸Šé¢ä»£ç æ²¡æŠ›å‡ºå¼‚å¸¸ï¼Œå°±ç®—é€šè¿‡
    print("EXECUTION_PASSED")
except Exception as e:
    print(f"EXECUTION_FAILED: {{e}}")
"""
        return run_python_script(script)

    # --- 3. MCQ / Match ---
    elif task_type in ['mcq', 'match']:
        pred = extract_choice_answer(response)
        gt = str(item['gt']).strip().replace("(", "").replace(")", "")

        if not pred:
            # Fallback to math extraction just in case
            pred = extract_math_answer(response)

        is_correct = (pred.lower() == gt.lower())
        return is_correct, f"Pred: {pred} | GT: {gt}"

    # --- 4. Math ---
    else:  # math
        pred = extract_math_answer(response)
        gt = str(item['gt'])

        # ä½¿ç”¨ä¸¥æ ¼ç­‰ä»·æ€§åˆ¤å®š
        is_correct = is_equiv_math(pred, gt)
        return is_correct, f"Pred: {pred} | GT: {gt}"


# ================= 5. æ•°æ®åŠ è½½ä¸è·¯å¾„ =================

LOCAL_DATA_MAP = {
    "math500": "{data_root}/MATH-500/test.jsonl",
    "aime24": "{data_root}/AIME-2024/data/*.parquet",
    "aime25": "{data_root}/AIME-2025/aime2025-*.jsonl",
    "olympiad": "{data_root}/OlympiadBench/OlympiadBench/OE_TO_maths_en_COMP/*.parquet",
    "gpqa": "{data_root}/GPQA-Diamond/test/gpqa_diamond.parquet",
    # BBH Top-3
    "bbh": [
        "{data_root}/big_bench_hard/logical_deduction_seven_objects/*.parquet",
        "{data_root}/big_bench_hard/date_understanding/*.parquet",
        "{data_root}/big_bench_hard/boolean_expressions/*.parquet"
    ],
    "humaneval": "{data_root}/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet",
    "lcb": "{data_root}/LiveCodeBench/test5.jsonl",
    "leetcode": "{data_root}/LeetCodeDataset/LeetCodeDataset-test.jsonl"
}


def load_local_data(task, data_root, limit=-1):
    raw_template = LOCAL_DATA_MAP.get(task)
    if not raw_template:
        print(f"Unknown task: {task}")
        return []

    if isinstance(raw_template, list):
        path_list = [p.format(data_root=data_root) for p in raw_template]
    else:
        path_list = [raw_template.format(data_root=data_root)]

    files = []
    for full_path in path_list:
        if '*' in full_path:
            files.extend(sorted(glob.glob(full_path, recursive=True)))
        elif os.path.exists(full_path):
            files.append(full_path)

    if not files:
        print(f"âŒ No files found for {task}")
        return []

    dfs = []
    for f in files:
        try:
            if f.endswith('.parquet'):
                dfs.append(pd.read_parquet(f))
            elif f.endswith('.jsonl'):
                dfs.append(pd.read_json(f, lines=True))
        except Exception as e:
            print(f"âš ï¸ Error reading {f}: {e}")

    if not dfs: return []
    df = pd.concat(dfs, ignore_index=True)

    data = []
    for idx, row in df.iterrows():
        try:
            item = {"id": f"{task}_{idx}", "type": "unknown", "gt": ""}

            # --- Math ---
            if task == "math500":
                item.update({"prompt": row['problem'], "gt": row['solution'], "type": "math"})
            elif task == "aime24":
                item.update({"prompt": row['problem'], "gt": row['solution'], "type": "math"})
            elif task == "aime25":
                item.update({"prompt": row['question'], "gt": str(row['answer']), "type": "math"})
            elif task == "olympiad":
                if row.get('question_type') != 'Open-ended': continue
                gt = row['solution']
                if isinstance(row.get('final_answer'), list) and len(row['final_answer']) > 0:
                    gt = row['final_answer'][0]
                item.update({"prompt": row['question'], "gt": gt, "type": "math"})

            # --- MCQ ---
            elif task == "gpqa":
                item.update({"prompt": row['Question'], "gt": row['Correct Answer'], "type": "mcq"})
            elif task == "bbh":
                item.update({"prompt": row['question'], "gt": row['target'], "type": "match"})

            # --- Code ---
            elif task == "humaneval":
                item.update(
                    {"prompt": row['prompt'], "gt": row['test'], "entry": row['entry_point'], "type": "code_script"})

            # [LBC IO Fix]
            elif task == "lcb":
                # å°è¯•è§£æ Input/Output
                # LCB æ•°æ®æ ¼å¼å„å¼‚ï¼Œè¿™é‡Œå‡è®¾æœ‰ public_test_cases å­—æ®µ (å¸¸è§æ ¼å¼)
                inputs, outputs = [], []
                if 'public_test_cases' in row:
                    try:
                        # å‡è®¾å®ƒæ˜¯ JSON å­—ç¬¦ä¸²æˆ–è€…å·²ç»æ˜¯ list
                        cases = row['public_test_cases']
                        if isinstance(cases, str): cases = json.loads(cases)
                        for c in cases:  # c is dict {input:..., output:...}
                            inputs.append(c.get('input', ''))
                            outputs.append(c.get('output', ''))
                    except:
                        pass

                item.update({
                    "prompt": row['question_content'],
                    "gt": "io_check",
                    "type": "code_io",
                    "test_inputs": inputs,
                    "test_outputs": outputs
                })

            elif task == "leetcode":
                item.update({"prompt": row['prompt'], "gt": row.get('test', ''), "type": "code_script"})

            data.append(item)
        except Exception:
            pass

    if limit > 0: data = data[:limit]
    return data


def estimate_pass_k(n, c, k):
    if n < k: return 0.0
    if c == 0: return 0.0
    if c == n: return 1.0
    prod = 1.0
    for i in range(k): prod *= (n - c - i) / (n - i)
    return 1.0 - prod


# ================= 6. å¤šè¿›ç¨‹è¯„ä¼° Wrapper =================

def process_single_item(args):
    """ç”¨äºå¤šè¿›ç¨‹çš„ worker å‡½æ•°"""
    response, item = args
    ok, msg = check_sample(response, item)
    return ok, msg


# ================= 7. ä¸»ç¨‹åº =================

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", required=True)
    parser.add_argument("--model_alias", required=True)
    parser.add_argument("--data_root", required=True)
    parser.add_argument("--tasks", default="math500")
    parser.add_argument("--output_dir", required=True)
    parser.add_argument("--k_values", default="1,4,8,16")
    parser.add_argument("--limit", type=int, default=-1)
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.9)
    parser.add_argument("--tp_size", type=int, default=1)
    return parser.parse_args()


def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    k_list = sorted([int(x) for x in args.k_values.split(",")])
    max_k = max(k_list)

    print(f"ğŸš€ Loading {args.model_alias} | Mem: {args.gpu_memory_utilization}")
    llm = LLM(model=args.model_path, tensor_parallel_size=args.tp_size,
              trust_remote_code=True, gpu_memory_utilization=args.gpu_memory_utilization, enforce_eager=True)

    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    except:
        print("Warning: Using GPT2 tokenizer fallback")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")

    summary = []

    for task in args.tasks.split(","):
        detail_path = os.path.join(args.output_dir, f"{args.model_alias}_{task}_details.jsonl")

        # æ–­ç‚¹ç»­ä¼ ï¼šç®€å•çš„è·³è¿‡é€»è¾‘
        if os.path.exists(detail_path):
            print(f"â© Skipping {task}, file exists: {detail_path}")
            # è¿™é‡Œç®€å•å¤„ç†ï¼Œå¦‚æœéœ€è¦åˆå¹¶ summary éœ€é¢å¤–è¯»å–
            continue

        try:
            data = load_local_data(task, args.data_root, args.limit)
            if not data:
                print(f"âš ï¸ Skipping {task}: No data loaded.")
                continue

            print(f"\nEvaluating {task} ({len(data)} items)...")
            prompts = []
            for item in data:
                sys_msg = "You are a helpful assistant."
                if item['type'] in ['code_script', 'code_io']:
                    sys_msg = "Write Python code to solve the problem. Wrap code in ```python ... ```."
                if item['type'] == 'math':
                    sys_msg = "Reason step by step. Finally, enclose the answer in \\boxed{}."
                if item['type'] in ['mcq', 'match']:
                    sys_msg = "Think step by step. Finally, answer with the option letter (e.g., (A)) or statement."

                # Chat Template æ„é€ 
                if getattr(tokenizer, 'chat_template', None):
                    p = tokenizer.apply_chat_template([
                        {"role": "system", "content": sys_msg},
                        {"role": "user", "content": item['prompt']}
                    ], tokenize=False, add_generation_prompt=True)
                else:
                    p = f"<|im_start|>system\n{sys_msg}<|im_end|>\n<|im_start|>user\n{item['prompt']}<|im_end|>\n<|im_start|>assistant\n"
                prompts.append(p)

            # ----------------------------------------------------
            # 1. Greedy Pass
            # ----------------------------------------------------
            print("  > Greedy Inference...")
            out_g = llm.generate(prompts, SamplingParams(temperature=0.0, max_tokens=4096))

            greedy_res_list = [o.outputs[0].text for o in out_g]

            # å¹¶è¡Œåˆ¤åˆ†
            print("  > Grading Greedy...")
            details = []
            greedy_correct = 0

            # æ„é€ ä»»åŠ¡åˆ—è¡¨
            grade_tasks = list(zip(greedy_res_list, data))

            with ProcessPoolExecutor(max_workers=min(32, os.cpu_count())) as executor:
                results = list(tqdm(executor.map(process_single_item, grade_tasks), total=len(grade_tasks)))

            for i, (ok, msg) in enumerate(results):
                if ok: greedy_correct += 1
                details.append({
                    "id": data[i]['id'],
                    "prompt": data[i]['prompt'],
                    "gt": str(data[i]['gt'])[:100],  # æˆªæ–­ä¸€ä¸‹é˜²æ­¢è¿‡å¤§
                    "greedy_res": greedy_res_list[i],
                    "greedy_ok": ok,
                    "info": msg
                })

            acc_g = greedy_correct / len(data)
            print(f"    Greedy: {acc_g:.2%}")

            # ----------------------------------------------------
            # 2. Sampling Pass
            # ----------------------------------------------------
            print(f"  > Sampling N={max_k}...")
            out_s = llm.generate(prompts, SamplingParams(temperature=0.6, top_p=0.95, n=max_k, max_tokens=4096))

            pass_k_scores = {k: [] for k in k_list}

            print("  > Grading Samples...")
            # ç”±äºæ•°æ®é‡å¤§ï¼Œè¿™é‡Œéœ€è¦æŠŠ flatten åçš„ sample éƒ½é€å…¥ executor
            # ç»“æ„: [ (sample_text, item), ... ]
            flat_tasks = []
            task_indices = []  # è®°å½•å±äºå“ªä¸ªé¢˜ç›®

            for i, o in enumerate(out_s):
                for sample in o.outputs:
                    flat_tasks.append((sample.text, data[i]))
                    task_indices.append(i)

            with ProcessPoolExecutor(max_workers=min(32, os.cpu_count())) as executor:
                flat_results = list(tqdm(executor.map(process_single_item, flat_tasks), total=len(flat_tasks)))

            # èšåˆç»“æœ
            sample_correct_counts = [0] * len(data)
            for idx, (ok, _) in zip(task_indices, flat_results):
                if ok: sample_correct_counts[idx] += 1

            # è®¡ç®— Pass@K
            for i, c_cnt in enumerate(sample_correct_counts):
                for k in k_list:
                    pass_k_scores[k].append(estimate_pass_k(max_k, c_cnt, k))

                # æ›´æ–° details
                details[i].update({
                    "sample_correct_cnt": c_cnt,
                    "samples_preview": [flat_tasks[j][0][:100] for j in range(len(flat_tasks)) if task_indices[j] == i][
                                       :3]
                })

            # Save
            pd.DataFrame(details).to_json(detail_path, orient='records', lines=True)
            print(f"    ğŸ’¾ Saved details to: {detail_path}")

            row = {"model": args.model_alias, "task": task, "greedy": acc_g}
            for k in k_list:
                avg = np.mean(pass_k_scores[k])
                row[f"pass@{k}"] = avg
                print(f"    Pass@{k}: {avg:.2%}")
            summary.append(row)

        except Exception as e:
            print(f"âŒ CRITICAL ERROR in {task}: {e}")
            import traceback
            traceback.print_exc()

    pd.DataFrame(summary).to_csv(os.path.join(args.output_dir, f"{args.model_alias}_summary.csv"), index=False)


if __name__ == "__main__":
    # é˜²æ­¢å¤šè¿›ç¨‹æ­»é”
    multiprocessing.set_start_method('spawn', force=True)
    main()