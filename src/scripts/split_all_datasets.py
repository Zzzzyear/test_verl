import os
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from tqdm import tqdm

# ================= é…ç½®åŒº =================
DATA_ROOT = "/data/zhaoqn/workspace/EGPO/datasets/processed"

TASKS = [
    ("mixed_reasoning.parquet", 256),
    ("math_single.parquet", 128),
    ("code_single.parquet", 128)
]

def get_large_type(data_type):
    """é€’å½’å°†ç±»å‹å‡çº§ä¸º 64ä½ Large ç±»å‹"""
    if pa.types.is_string(data_type): return pa.large_string()
    if pa.types.is_binary(data_type): return pa.large_binary()
    if pa.types.is_list(data_type): return pa.large_list(get_large_type(data_type.value_type))
    if pa.types.is_fixed_size_list(data_type): return pa.fixed_size_list(get_large_type(data_type.value_type), data_type.list_size)
    if pa.types.is_struct(data_type):
        return pa.struct([field.with_type(get_large_type(field.type)) for field in data_type])
    if pa.types.is_map(data_type):
        return pa.map_(get_large_type(data_type.key_type), get_large_type(data_type.item_type))
    return data_type

def get_safe_schema(original_schema):
    """æ ¹æ®åŸå§‹ Schema ç”Ÿæˆå…¨ 64ä½ çš„å®‰å…¨ Schema"""
    new_fields = []
    for field in original_schema:
        new_type = get_large_type(field.type)
        new_fields.append(field.with_type(new_type))
    return pa.schema(new_fields)

def process_and_write(table, indices, output_path, chunk_size=1000, desc="Writing"):
    """åˆ†å—æå–å¹¶å†™å…¥"""
    total = len(indices)
    if total == 0: return

    # ä½¿ç”¨ table è‡ªèº«çš„ schema (å·²ç»æ˜¯ safe çš„äº†)
    with pq.ParquetWriter(output_path, table.schema) as writer:
        for start in tqdm(range(0, total, chunk_size), desc=desc, unit="chunk"):
            end = min(start + chunk_size, total)
            batch_indices = indices[start:end]
            
            # æå–
            batch = table.take(batch_indices)
            
            # å†™å…¥
            writer.write_table(batch)

def split_and_save_ultimate():
    print(f"ğŸš€ Starting Read-Time Schema Evolution split in: {DATA_ROOT}")
    print("=" * 60)

    for filename, val_size in TASKS:
        source_path = os.path.join(DATA_ROOT, filename)
        
        if not os.path.exists(source_path):
            print(f"âš ï¸  File not found: {filename}, skipping...")
            continue

        print(f"ğŸ“– Analyzing {filename}...")
        try:
            # 1. [å…³é”®] åªè¯»å– Metadata (Schema)ï¼Œä¸è¯»å–æ•°æ®
            original_schema = pq.read_schema(source_path)
            
            # 2. [å…³é”®] æ„å»ºç›®æ ‡ Safe Schema (å…¨ Large ç±»å‹)
            safe_schema = get_safe_schema(original_schema)
            
            # 3. [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨ safe_schema è¯»å–æ–‡ä»¶
            # è¿™ä¸€æ­¥ä¼šè¿«ä½¿ PyArrow åœ¨ä»ç£ç›˜åŠ è½½æ•°æ®æ—¶ï¼Œç›´æ¥æ„å»º 64ä½ æ•°ç»„
            # ä»è€Œè·³è¿‡äº†é‚£ä¸ªè„†å¼±çš„ 32ä½ è½¬æ¢è¿‡ç¨‹ï¼Œå½»åº•æ ¹æ²» Overflow
            table = pq.read_table(source_path, schema=safe_schema)
            
        except Exception as e:
            print(f"âŒ Error during safe load: {e}")
            continue

        total_len = table.num_rows
        if total_len <= val_size:
            print(f"âŒ Too small ({total_len} <= {val_size}), skipping.")
            continue

        # ç´¢å¼•æ“ä½œ
        indices = np.arange(total_len)
        rng = np.random.default_rng(seed=42)
        rng.shuffle(indices)
        
        val_indices = indices[:val_size]
        train_indices = indices[val_size:]
        
        # è·¯å¾„
        base_name = filename.replace(".parquet", "")
        train_out = os.path.join(DATA_ROOT, f"{base_name}_train_final.parquet")
        val_out = os.path.join(DATA_ROOT, f"{base_name}_val_fixed.parquet")

        print(f"   Task: {base_name} | Total: {total_len}")
        print(f"   Schema upgraded to Large types? Yes.")
        
        try:
            # å†™å…¥éªŒè¯é›†
            process_and_write(table, val_indices, val_out, desc="   [1/2] Validation")
            
            # å†™å…¥è®­ç»ƒé›†
            process_and_write(table, train_indices, train_out, desc="   [2/2] Training  ")
            
        except Exception as e:
            print(f"\nâŒ Error during write: {e}")
            continue

        print(f"   âœ… Done.\n")

    print("ğŸ‰ All tasks finished. You are now Overflow-Proof.")

if __name__ == "__main__":
    split_and_save_ultimate()