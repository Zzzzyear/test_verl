import os
import glob
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import AutoTokenizer
from datasets import load_dataset
import numpy as np
from tqdm import tqdm

# ================= é…ç½®åŒºåŸŸ =================
BASE_DIR = "/data/zhaoqn/workspace/EGPO"
RAW_DIR = os.path.join(BASE_DIR, "datasets/raw")
OUTPUT_IMG_DIR = os.path.join(BASE_DIR, "outputs/analysis")
os.makedirs(OUTPUT_IMG_DIR, exist_ok=True)

# æœ¬åœ°æ¨¡å‹è·¯å¾„
TOKENIZER_PATH = "/data/zhaoqn/models/Qwen/Qwen3-8B"

# é‡‡æ ·æ•°é‡ (è®¾ä¸º None åˆ™åˆ†æå…¨é‡ï¼Œå»ºè®®å…ˆè·‘ 10000 æ¡å¿«é€Ÿçœ‹ç»“æœ)
SAMPLE_SIZE = 10000 

# å®šä¹‰æˆ‘ä»¬è¦åˆ†æçš„æ•°æ®é›†åŠå…¶æ ¼å¼ç‰¹å¾
TARGETS = {
    "Mixture-of-Thoughts": {
        "path": "Mixture-of-Thoughts/**/*.parquet",
        "format": "parquet",
        "type": "messages" # å­—æ®µæ˜¯ messages åˆ—è¡¨
    },
    "NuminaMath": {
        "path": "NuminaMath-CoT/**/*.parquet",
        "format": "parquet",
        "type": "col_prob_sol" # å­—æ®µæ˜¯ problem + solution
    },
    "Eurus-2": {
        "path": "Eurus-2-RL-Data/**/*.parquet",
        "format": "parquet",
        "type": "eurus" # ç‰¹æ®Š: prompt(list) + response/solution
    },
    "MATH-500 (Test)": {
        "path": "MATH-500/**/*.jsonl",
        "format": "json",
        "type": "col_prob_sol"
    },
    "AIME-2024 (Test)": {
        "path": "AIME-2024/**/*.parquet",
        "format": "parquet",
        "type": "col_prob_sol"
    }
}

# ================= å·¥å…·å‡½æ•° =================

def get_tokenizer():
    try:
        print(f"ğŸ”„ Loading tokenizer from {TOKENIZER_PATH} ...")
        return AutoTokenizer.from_pretrained(TOKENIZER_PATH, trust_remote_code=True)
    except Exception as e:
        print(f"âš ï¸ Load local tokenizer failed: {e}. Trying gpt2...")
        return AutoTokenizer.from_pretrained("gpt2")

tokenizer = get_tokenizer()

def extract_text(example, data_type):
    """æ ¹æ®æ•°æ®é›†ç±»å‹æå–å®Œæ•´çš„ prompt + response æ–‡æœ¬"""
    text = ""
    
    try:
        # Type 1: Standard Messages (Mixture-of-Thoughts)
        if data_type == "messages":
            if 'messages' in example and isinstance(example['messages'], list):
                for msg in example['messages']:
                    if isinstance(msg, dict) and 'content' in msg:
                        text += str(msg['content']) + " "
        
        # Type 2: Problem/Solution Columns (Numina, MATH-500, AIME)
        elif data_type == "col_prob_sol":
            # å°è¯•å¯»æ‰¾å¸¸è§çš„é¢˜ç›®åˆ—å
            p = example.get('problem') or example.get('question') or example.get('input') or ""
            # å°è¯•å¯»æ‰¾å¸¸è§çš„ç­”æ¡ˆåˆ—å
            r = example.get('solution') or example.get('answer') or example.get('output') or ""
            text = str(p) + " " + str(r)

        # Type 3: Eurus Special (Prompt is List[Dict])
        elif data_type == "eurus":
            # Eurus Prompt
            if 'prompt' in example and isinstance(example['prompt'], list):
                 for msg in example['prompt']:
                    if isinstance(msg, dict) and 'content' in msg:
                        text += str(msg['content']) + " "
            elif 'prompt' in example:
                text += str(example['prompt']) + " "
            
            # Eurus Response (Training set usually has it)
            if 'response' in example:
                text += str(example['response'])
            elif 'solution' in example:
                text += str(example['solution'])

    except Exception as e:
        return "" # è§£æå¤±è´¥è¿”å›ç©º

    return text

def calc_len_batch(examples, data_type):
    """æ‰¹é‡è®¡ç®—é•¿åº¦"""
    batch_texts = []
    # examples æ˜¯ä¸€ä¸ª dict: {'col1': [v1, v2], 'col2': [v1, v2]}
    # æˆ‘ä»¬éœ€è¦å°†å…¶è½¬å› row æ ¼å¼æ¥å¤„ç†
    keys = list(examples.keys())
    num_rows = len(examples[keys[0]])
    
    for i in range(num_rows):
        # æ„é€ å•è¡Œ example dict
        row = {k: examples[k][i] for k in keys}
        txt = extract_text(row, data_type)
        batch_texts.append(txt)
    
    # æ‰¹é‡ Tokenize (é€Ÿåº¦å¿«)
    encodings = tokenizer(batch_texts, truncation=False, add_special_tokens=False)
    lengths = [len(ids) for ids in encodings['input_ids']]
    
    return {'num_tokens': lengths}

# ================= ä¸»é€»è¾‘ =================

def main():
    results = {}

    for name, config in TARGETS.items():
        print(f"\nğŸ“Š [Analyzing] {name} ...")
        path_pattern = os.path.join(RAW_DIR, config['path'])
        files = glob.glob(path_pattern, recursive=True)
        
        if not files:
            print(f"   âŒ File not found: {path_pattern}")
            continue

        try:
            # åŠ è½½æ•°æ®
            ds = load_dataset(config['format'], data_files=files, split="train")
            
            if SAMPLE_SIZE and len(ds) > SAMPLE_SIZE:
                ds = ds.select(range(SAMPLE_SIZE))
            
            # è®¡ç®—é•¿åº¦
            # ä½¿ç”¨ fn_kwargs ä¼ é€’ data_type
            ds = ds.map(
                lambda x: calc_len_batch(x, config['type']), 
                batched=True, 
                batch_size=1000,
                desc="   Tokenizing"
            )
            
            # æå–æœ‰æ•ˆé•¿åº¦
            lens = [l for l in ds['num_tokens'] if l > 0]
            if lens:
                results[name] = pd.Series(lens, name=name)
            else:
                print(f"   âš ï¸  No valid tokens found (schema mismatch?)")

        except Exception as e:
            print(f"   âŒ Error: {e}")

    # --- æ‰“å°æŠ¥å‘Š ---
    print("\n" + "="*85)
    print(f"{'Dataset':<25} | {'Count':<8} | {'Min':<6} | {'Median':<6} | {'Max':<8} | {'P5':<6} | {'P95':<6}")
    print("-" * 85)

    for name, s in results.items():
        if len(s) == 0: continue
        p5 = int(s.quantile(0.05))
        p95 = int(s.quantile(0.95))
        median = int(s.median())
        print(f"{name:<25} | {len(s):<8} | {s.min():<6} | {median:<6} | {s.max():<8} | {p5:<6} | {p95:<6}")
    print("="*85 + "\n")

    # --- ç»˜å›¾ ---
    if results:
        print(f"ğŸ¨ Drawing distribution plot to {OUTPUT_IMG_DIR} ...")
        plt.figure(figsize=(14, 7))
        
        for name, s in results.items():
            # æˆªæ–­æå€¼ä»¥ä¾¿ç»˜å›¾æ¸…æ™° (åªç”» 98% çš„æ•°æ®)
            cutoff = s.quantile(0.98)
            subset = s[s < cutoff]
            sns.kdeplot(subset, label=f"{name} (Med: {int(s.median())})", fill=True, alpha=0.3)
        
        plt.title(f"Token Length Distribution (Truncated at P98) - Tokenizer: {os.path.basename(TOKENIZER_PATH)}")
        plt.xlabel("Number of Tokens")
        plt.xlim(0, None) # ä» 0 å¼€å§‹
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        save_path = os.path.join(OUTPUT_IMG_DIR, "token_distribution_v3.png")
        plt.savefig(save_path)
        print(f"âœ… Plot saved: {save_path}")

if __name__ == "__main__":
    main()